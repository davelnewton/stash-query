#!/usr/bin/env ruby

require 'elasticsearch'

require 'json'
require 'date'
require 'optparse'
require 'progress_bar'
require 'curb'
require 'pry'

############ CONFIG ###########

$config = {}
# $config[:host] = "localhost"
$config[:host] = "localhost"
$config[:port] = "9200"
$config[:index_prefix] = "logstash-"
$config[:scroll_size] = 10  ## Number of hits returned per scroll request. Not sure what to use here...
$config[:scroll_time] = '30m'   
$config[:report] = nil

$debug = false
$flush_buffer = 1000 ## Number of log lines to flush to file at

##################################

# TODO Real CSV lib?
def flush_to_file(hit_list)
  File.open($config[:output], 'a') do |file|
    file.puts(hit_list)
  end
rescue => e
  puts "Error writing to output file #{$config[:output]}"
  raise e
end

def validate_date(str)
  !!(str =~ /20[0-9]{2}-(0[1-9]|1[012])-(0[1-9]|[12][0-9]|3[01])T[012][0-9]:[0-5][0-9]:[0-5][0-9]\.[0-9]{3}Z/)
end

## Prints numbers with commas
def print_num(min)
  min.to_s.reverse.gsub(/(\d{3})(?=\d)/, '\\1,').reverse
end

OptionParser.new do |opts|
  opts.banner = "Usage: "
  opts.on('-c','--connect_host [HOST]', "Logstash host to run query on (defaults to: #{$config[:host]})") { |v| $config[:host] = v unless v.empty? or v.nil? }
  opts.on('-p','--port [PORT]', "Logstash port (defaults to: #{$config[:port]})") { |v| $config[:port] = v unless v.empty? or v.nil? }
  opts.on('-i','--index-prefix [PREFIX]', "Index name prefix. Defaults to 'logstash-'") { |v| $config[:index_prefix] = v unless v.empty? or v.nil? }
  opts.on('-w', '--write [FILE]', 'Write output file location (defaults to nil)') { |v| $config[:output] = v }
  opts.on('-d', '--debug', 'Debug mode') { |v| $debug = true }

  opts.on('-s', '--start [DATE]', 'Start date. Format: YYYY-MM-DDThh:mm:ss.SSSZ. Ex: 2013-12-01T12:00:00.000Z') do |v| 
    if validate_date(v)
      $config[:start] = v
    else
      puts "Incorrect timestamp format for start date"
      exit
    end
  end

  opts.on('-e', '--end [DATE]', 'End date. Format: YYYY-MM-DDThh:mm:ss.SSSZ') do |v| 
    if validate_date(v)
      $config[:end] = v
    else
      puts "Incorrect timestamp format for end date"
      exit
    end
  end

  opts.on('-q', '--query [QUERY]', 'Query string') { |v| $config[:query] = "#{v}" unless v.empty? }
  opts.on('-t', '--tags [TAGS]', 'Tags to query. Comma delimited')  do |tags|  
    arr = tags.split(',')
    if arr.length > 1
      $config[:tags] = "tags:(#{arr.join(' AND ')})"
    else
      $config[:tags] = "tags:#{tags}"
    end
  end
  opts.parse!
end

## Cleanup output file. Probably a better way to do this.
begin
  File.truncate($config[:output], 0)
rescue
end

def get_es_client(options={})
  Elasticsearch::Client.new(host: options[:host], port: options[:port])
rescue => e
  puts e.inspect
  raise "Could not connect to ES: #{options[:host]}:#{options[:port]}"
end

es = get_es_client($config)
puts "Connected to ES" if $debug

time_range = "@timestamp:[#{$config[:start]} TO #{$config[:end]}]"

queries = []
queries << "#{$config[:tags]}"  if $config[:tags]
queries << "#{$config[:query]}" if $config[:query] 
queries << "#{time_range}" if $config[:start] and $config[:end]
query = queries.join(' AND ')

puts "Query is '#{query}'" if $debug

# TODO Refactor date stuff
def get_indices(options={})
  if options[:start] && options[:end]
    puts "Have start/end, getting indices..." if $debug
    start_str  = options[:start].split('T').first.split('-').join('.')
    s_year     = start_str.split('.').first.to_i
    s_mo       = start_str.split('.')[1].to_i
    s_day      = start_str.split('.').last.to_i
    start_date = Date.new(s_year, s_mo, s_day)

    end_str  = options[:end].split('T').first.split('-').join('.')
    e_year   = end_str.split('.').first.to_i
    e_mo     = end_str.split('.')[1].to_i
    e_day    = end_str.split('.').last.to_i
    end_date = Date.new(e_year, e_mo, e_day)

    (start_date..end_date).collect do |day|
      day = day.strftime('%Y.%m.%d')
      puts "DAY: #{day}" if $debug
      "#{$config[:index_prefix]}#{day}"
    end
  else
    puts "You have not specified a start and/or end timestamp for your query"
    puts "I will default to search all existing indices. This will cause"
    puts "the query to be extremely slow. Shall I continue? (y/n) "
    ans = gets 
    exit unless ans.downcase =~ /y/
    ['_all']
  end
end

indexes = get_indices($config)

## Make sure each index exists
# TODO find_all?
good_indexes = []
unless indexes.include?('_all')
  indexes.each do |index|
    good_indexes << index if es.indices.exists index: index
  end
  indexes = good_indexes
else
  indexes = [ '_all' ]
end

puts "Using these indices: #{indexes.join(',')}" if $debug

# Get scroll ID
puts "Running this query:"
puts "    #{query}"
index_str = indexes.join(',')
res       = es.search index: index_str, q: query, search_type: 'scan', scroll: $config[:scroll_time], size: $config[:scroll_size], df: 'message'

scroll_id = res['_scroll_id']

scroll_ids = [res['_scroll_id']]

puts "Your query returns #{res['hits']['total']} results"
puts

puts res.inspect if $debug and res['hits']['total'] > 300000

if $config[:output]
  puts "Writing messages to file #{$config[:output]}..."
  bar = ProgressBar.new(res['hits']['total'])
  hit_list = ''
  total_lines = 0 if $debug
  while true
    res['hits']['hits'].each do |hit|
      source = hit['_source']
      # hit_list += hit['_source']['message']
      hit_list += "#{source['user']},#{source['niCodes']},#{source['message']}\n"
      if hit_list.lines.count % $flush_buffer == 0
        flush_to_file hit_list
        hit_list = ''
      end
    end

    bar.increment! res['hits']['hits'].length
    total_lines += res['hits']['hits'].length if $debug

    # Continue scroll through data
    begin
      res = es.scroll scroll: $config[:scroll_time], body: scroll_id
      scroll_id = res['_scroll_id']
      scroll_ids << res['_scroll_id']
    rescue => e
      puts "EXCEPTION"
      puts res.inspect
      raise e
    end

    begin
      break if res['hits']['hits'].length < 1
    rescue => e
      puts "RESULT TYPE: #{res.class}"
      puts res.inspect
      raise e
    end
  end

  flush_to_file hit_list

  ## Delete the scroll_ids to free up resources on the ES cluster
  ## Have to use direct API call until elasticsearch-ruby supports this
  # TODO See if this is supported
  scroll_ids.uniq.each do |scroll|
    puts "DELETE SCROLL:#{scroll}" if $debug
    puts
    begin
      Curl.delete("#{$config[:host]}:#{$config[:port]}/_search/scroll/#{scroll}")
    rescue
      puts "Delete failed" if $debug
    end
  end
end
